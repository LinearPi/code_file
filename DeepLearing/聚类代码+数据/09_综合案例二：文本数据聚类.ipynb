{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载的20新闻数据中的数据类别为: ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "3387条数据；4个新闻类别\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "print u'加载的20新闻数据中的数据类别为:',categories\n",
    "\n",
    "\n",
    "dataset = fetch_20newsgroups(data_home='datas', subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "print(\"%d条数据；%d个新闻类别\" % (len(dataset.data), len(dataset.target_names)))\n",
    "\n",
    "\n",
    "labels = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_cluster_k = np.unique(labels).shape[0]\n",
    "features = 2 ** 20\n",
    "components = 5\n",
    "mini_batch_km_batchsize = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "hasher1 = HashingVectorizer(n_features=features, stop_words='english', non_negative=True, \n",
    "                            norm=None, binary=False, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "tt = TfidfTransformer(norm='l2', use_idf=True)\n",
    "hasher2 = HashingVectorizer(n_features=features, stop_words='english', non_negative=False,\n",
    "                            norm='l2', binary=False, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "tv = TfidfVectorizer(max_df=0.5, max_features=features, min_df=2, stop_words='english', use_idf=True)\n",
    "\n",
    "\n",
    "vectorizers = [\n",
    "    ('hashing&tf-idf', make_pipeline(hasher1, tt), False),\n",
    "    ('hasing', make_pipeline(hasher2), False),\n",
    "    ('tf-idf', make_pipeline(tv), True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=components)\n",
    "normalizer = Normalizer(norm='l2', copy=False)\n",
    "sn = make_pipeline(svd, normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "mbkm = MiniBatchKMeans(n_clusters=target_cluster_k, init='k-means++', n_init=5, \n",
    "                       init_size=10 * mini_batch_km_batchsize, batch_size=mini_batch_km_batchsize)\n",
    "\n",
    "km = KMeans(n_clusters=target_cluster_k, init='k-means++', max_iter=100, n_init=5)\n",
    "\n",
    "cluster_als = [('Mini-Batch-KMeans', mbkm), ('KMeans', km)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "采用'hashing&tf-idf'的方式将文本数据转换为特征矩阵\n",
      "转换消耗时间:2.151s\n",
      "样本数量:3387,特征属性数量:1048576\n",
      "SVD分解及归一化消耗时间:7.753s\n",
      "降维&归一化操作后，样本数量:3387,特征属性数量:5\n",
      "\n",
      "使用算法Mini-Batch-KMeans对数据进行建模操作\n",
      "模型构建消耗时间:0.044s\n",
      "Mini-Batch-KMeans算法效果评估相关系数\n",
      "均一性/同质性: 0.553\n",
      "完整性: 0.594\n",
      "V-measure: 0.573\n",
      "Adjusted Rand-Index(ARI): 0.554\n",
      "轮廓系数: 0.389\n",
      "聚类中心点为: [[ 0.81965113  0.23784285 -0.11635794 -0.15431643 -0.17500972]\n",
      " [ 0.7198625  -0.53206199 -0.05900797  0.12853194  0.03868587]\n",
      " [ 0.76368709 -0.2896739   0.00599186 -0.36158345  0.33797136]\n",
      " [ 0.67635775  0.28099513  0.52946783  0.16911658  0.07248387]]\n",
      "\n",
      "使用算法KMeans对数据进行建模操作\n",
      "模型构建消耗时间:0.057s\n",
      "KMeans算法效果评估相关系数\n",
      "均一性/同质性: 0.563\n",
      "完整性: 0.593\n",
      "V-measure: 0.578\n",
      "Adjusted Rand-Index(ARI): 0.574\n",
      "轮廓系数: 0.395\n",
      "聚类中心点为: [[ 0.75261424 -0.29979432  0.00810426 -0.37995294  0.35494373]\n",
      " [ 0.73036399 -0.52309952 -0.05496925  0.1201719   0.04725732]\n",
      " [ 0.86294126  0.20612099 -0.02661772 -0.20769458 -0.22264159]\n",
      " [ 0.58541422  0.33279894  0.16698756  0.27796689  0.21453899]]\n",
      "\n",
      "\n",
      "============================================\n",
      "采用'hasing'的方式将文本数据转换为特征矩阵\n",
      "转换消耗时间:1.931s\n",
      "样本数量:3387,特征属性数量:1048576\n",
      "SVD分解及归一化消耗时间:8.250s\n",
      "降维&归一化操作后，样本数量:3387,特征属性数量:5\n",
      "\n",
      "使用算法Mini-Batch-KMeans对数据进行建模操作\n",
      "模型构建消耗时间:0.042s\n",
      "Mini-Batch-KMeans算法效果评估相关系数\n",
      "均一性/同质性: 0.152\n",
      "完整性: 0.156\n",
      "V-measure: 0.154\n",
      "Adjusted Rand-Index(ARI): 0.095\n",
      "轮廓系数: 0.302\n",
      "聚类中心点为: [[ 0.72617492  0.35752617  0.33010835 -0.29788489 -0.10429142]\n",
      " [ 0.88283575 -0.24899841  0.0316642   0.0105184  -0.01577371]\n",
      " [ 0.66298835  0.09232763 -0.43976282 -0.28059383 -0.13917647]\n",
      " [ 0.74502967  0.43397472 -0.14347789  0.25689186  0.03706345]]\n",
      "\n",
      "使用算法KMeans对数据进行建模操作\n",
      "模型构建消耗时间:0.131s\n",
      "KMeans算法效果评估相关系数\n",
      "均一性/同质性: 0.154\n",
      "完整性: 0.158\n",
      "V-measure: 0.156\n",
      "Adjusted Rand-Index(ARI): 0.102\n",
      "轮廓系数: 0.308\n",
      "聚类中心点为: [[ 0.73235956  0.34958894  0.331555   -0.29753157 -0.09509105]\n",
      " [ 0.67160287  0.09758433 -0.43774181 -0.24751598 -0.19290196]\n",
      " [ 0.76442757  0.39428108 -0.11645702  0.25343918  0.07028127]\n",
      " [ 0.88002433 -0.27133862  0.0344141  -0.00644067 -0.00533253]]\n",
      "\n",
      "\n",
      "============================================\n",
      "采用'tf-idf'的方式将文本数据转换为特征矩阵\n",
      "转换消耗时间:2.204s\n",
      "样本数量:3387,特征属性数量:24545\n",
      "SVD分解及归一化消耗时间:0.476s\n",
      "降维&归一化操作后，样本数量:3387,特征属性数量:5\n",
      "\n",
      "使用算法Mini-Batch-KMeans对数据进行建模操作\n",
      "模型构建消耗时间:0.078s\n",
      "Mini-Batch-KMeans算法效果评估相关系数\n",
      "均一性/同质性: 0.592\n",
      "完整性: 0.640\n",
      "V-measure: 0.615\n",
      "Adjusted Rand-Index(ARI): 0.613\n",
      "轮廓系数: 0.441\n",
      "聚类中心点为: [[ 0.82182126  0.23424348 -0.10033154 -0.12660566 -0.20278622]\n",
      " [ 0.71481683 -0.54210505 -0.02136911  0.14415564  0.0403899 ]\n",
      " [ 0.70172624 -0.31896606  0.03242368 -0.47941545  0.31719222]\n",
      " [ 0.61011642  0.31761323  0.56577671  0.20015656  0.14155639]]\n",
      "获取文本转换特征矩阵中，各个分类考虑特征属性的前10个feature特征（10个单词）：\n",
      "类别0:  god  people  com  jesus  don  say  think  believe  bible  just\n",
      "类别1:  space  henry  toronto  nasa  access  com  digex  pat  gov  alaska\n",
      "类别2:  graphics  space  image  com  nasa  university  posting  program  images  file\n",
      "类别3:  sgi  livesey  keith  wpd  solntze  jon  com  caltech  morality  moral\n",
      "\n",
      "使用算法KMeans对数据进行建模操作\n",
      "模型构建消耗时间:0.097s\n",
      "KMeans算法效果评估相关系数\n",
      "均一性/同质性: 0.583\n",
      "完整性: 0.610\n",
      "V-measure: 0.596\n",
      "Adjusted Rand-Index(ARI): 0.599\n",
      "轮廓系数: 0.441\n",
      "聚类中心点为: [[ 0.71232982 -0.54459547 -0.02061554  0.15424596  0.03640805]\n",
      " [ 0.57572379  0.34187574  0.17066762  0.23292154  0.2570028 ]\n",
      " [ 0.85439056  0.21586654 -0.04367377 -0.16863448 -0.26838494]\n",
      " [ 0.70315518 -0.32380288  0.03251809 -0.47518394  0.31494816]]\n",
      "获取文本转换特征矩阵中，各个分类考虑特征属性的前10个feature特征（10个单词）：\n",
      "类别0:  space  henry  toronto  nasa  access  digex  com  pat  gov  zoo\n",
      "类别1:  com  sandvik  sgi  livesey  keith  wpd  solntze  jon  kent  caltech\n",
      "类别2:  god  people  jesus  don  com  say  believe  think  bible  just\n",
      "类别3:  graphics  space  image  com  nasa  university  posting  program  images  file\n",
      "\n",
      "\n",
      "==================算法完成======================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for vectorizer_name, vectorizer, can_inverse in vectorizers:\n",
    "    print \"============================================\"\n",
    "    print \"采用'%s'的方式将文本数据转换为特征矩阵\" % vectorizer_name\n",
    "\n",
    "    \n",
    "    t0 = time()\n",
    "    X = vectorizer.fit_transform(dataset.data)\n",
    "    print \"转换消耗时间:%.3fs\" % (time() - t0)\n",
    "    print \"样本数量:%d,特征属性数量:%d\" % X.shape\n",
    "\n",
    "   \n",
    "    t0 = time()\n",
    "    X = sn.fit_transform(X)\n",
    "    print \"SVD分解及归一化消耗时间:%.3fs\" % (time() - t0)\n",
    "    print \"降维&归一化操作后，样本数量:%d,特征属性数量:%d\" % X.shape\n",
    "    \n",
    "    \n",
    "    for cluster_name,cluster_al in cluster_als:\n",
    "        print\n",
    "        print \"使用算法%s对数据进行建模操作\" % cluster_name\n",
    "        t0 = time()\n",
    "        cluster_al.fit(X)\n",
    "        print \"模型构建消耗时间:%.3fs\" % (time() - t0)\n",
    "        print \"%s算法效果评估相关系数\" % cluster_name\n",
    "        print(u\"均一性/同质性: %0.3f\" % metrics.homogeneity_score(labels, cluster_al.labels_))\n",
    "        print(\"完整性: %0.3f\" % metrics.completeness_score(labels, cluster_al.labels_))\n",
    "        print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, cluster_al.labels_))\n",
    "        print(\"Adjusted Rand-Index(ARI): %.3f\" % metrics.adjusted_rand_score(labels, cluster_al.labels_))\n",
    "        print(\"轮廓系数: %0.3f\" % metrics.silhouette_score(X, cluster_al.labels_, sample_size=1000))\n",
    "        print \"聚类中心点为:\", cluster_al.cluster_centers_\n",
    "        \n",
    "        if can_inverse:\n",
    "            print \"获取文本转换特征矩阵中，各个分类考虑特征属性的前10个feature特征（10个单词）：\"\n",
    "            \n",
    "            original_space_centroids = svd.inverse_transform(cluster_al.cluster_centers_)\n",
    "            \n",
    "            order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "           \n",
    "            terms = vectorizer.named_steps.items()[0][1].get_feature_names()\n",
    "           \n",
    "            for i in range(target_cluster_k):\n",
    "                print \"类别%d:\" % i,\n",
    "                for ind in order_centroids[i, :10]:\n",
    "                    print ' %s' % terms[ind],\n",
    "                print\n",
    "    print\n",
    "    print\n",
    "print \"==================算法完成======================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
